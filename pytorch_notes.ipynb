{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62cbab2-d407-4864-a075-0ee77e4cd5bf",
   "metadata": {},
   "source": [
    "# A tutorial notes for Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42d54b-7a52-4d9d-a3c4-482dd0f5b17a",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://pytorch.org/tutorials/beginner/basics/intro.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb19d4c-fc57-46a0-8b3b-814ce3638d78",
   "metadata": {},
   "source": [
    "## introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6c9a2-7f6a-41b3-90ce-37cd918ef4f7",
   "metadata": {},
   "source": [
    "A introduction about the basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a20ede-d1b6-43ef-a755-ada2a8e5bfb3",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6defad5-fd7c-43c4-9efb-8ee2d5760329",
   "metadata": {},
   "source": [
    "what: \n",
    "\n",
    "A data structure to encode the inputs, outputs, and parameters of a model, which is similar to arrays and matrices.\n",
    "\n",
    "why: \n",
    "1. Tensors can run on GPUs or other hardware accelerators.\n",
    "2. Tensors share the same underlying memory with Numpy array, eliminating the need to copy data\n",
    "3. Tensors are optimized for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31d41bd-3938-42b2-86bf-bc3b760217e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719cb73-dd7a-4534-9510-0a6b4258c188",
   "metadata": {},
   "source": [
    "#### Initalize a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8338a661-990f-4885-9499-38a2cb0e896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3984010e-54b8-431e-9ea9-4affa96cdf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817b016f-5b9f-46d4-93bf-5e61851e9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a Numpy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a7a848-6b2a-4475-95d6-a85f47be51ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76026ad4-4061-4a29-a240-583d8bce2615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[0.7721, 0.0448],\n",
      "        [0.3881, 0.6359]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from another tensor, which keeps shape, datatype unless explicitly overridden.\n",
    "\n",
    "# retain the properties of x_data\n",
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"Ones Tensor: \\n {x_ones}\\n\")\n",
    "\n",
    "# overrides the datatype of x_data\n",
    "x_rand = torch.rand_like(x_data, dtype = torch.float) \n",
    "print(f\"Ones Tensor: \\n {x_rand}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "340763b3-31b2-4d6d-9a46-15cba78ec00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.2999, 0.4976, 0.9669],\n",
      "        [0.3437, 0.3316, 0.7742]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with shape \n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Random Tensor: \\n {zeros_tensor} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903297a8-52db-4ca7-8649-d19961e728c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor:torch.Size([3, 4])\n",
      "Datatype of tensor:torch.float32\n",
      "Device tensor is stored on:cpu\n"
     ]
    }
   ],
   "source": [
    "# attributes\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor:{tensor.shape}\")\n",
    "print(f\"Datatype of tensor:{tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on:{tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ec237-255d-4185-88ea-ef1045c4ed90",
   "metadata": {},
   "source": [
    "By default, tensors are created on the CPU.\n",
    "\n",
    "Copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4be5354-e6ac-487e-8fe4-ce81686bdfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c313415-4e6f-4bd7-88de-be0dfc85ea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bccc1d22-d0de-4223-a56b-200808ce32b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# bridge with Numpy (share the memory)\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59fe2dd3-0813-4d96-94c5-3b0d7f42d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4fb1752-f9fa-4713-9376-1d85d5d643b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# same for (numpy->torch tensor)\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "\n",
    "np.add(n,1,out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634eee44-86bc-4585-b610-9e0ac623d685",
   "metadata": {},
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88e8a167-c1b5-42ac-baf1-1423b410b692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column:  tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4,4)\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column: ' , tensor[...,-1])\n",
    "tensor[:, -1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ec189d5-f4de-490d-87c9-3f6851d30d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Joining tensors\n",
    "t1 = torch.cat([tensor,tensor,tensor], dim = 1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "016dfcb7-4e5b-4f67-9920-71375649b74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1: \n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n",
      "y2: \n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n",
      "y3: \n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# arithmetic operations\n",
    "# y1 = y2 = y3\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor,tensor.T,out=y3)\n",
    "# matrix multiplication\n",
    "print(f'y1: \\n{y1}\\n')\n",
    "print(f'y2: \\n{y2}\\n')\n",
    "print(f'y3: \\n{y3}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f643e0e-1c79-467e-a73f-ca1e637ccb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise product\n",
    "# z1, z2, z3 have the same value\n",
    "z1 = tensor * tensor\n",
    "\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor,tensor,out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a57dce5b-462e-47e0-869e-8d081ee2fc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single-element\n",
    "agg = tensor.sum()\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fca9cd1e-216c-4a77-9e87-b10a073725de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# to python numerical value\n",
    "print(agg.item(), type(agg.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "883e56a5-2005-4351-a2ef-8841fe3146ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.]]) \n",
      "\n",
      "tensor([[6., 6., 6., 5.],\n",
      "        [6., 6., 6., 5.],\n",
      "        [6., 6., 6., 5.],\n",
      "        [6., 6., 6., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# in-place operations denoted by _suffix\n",
    "print(tensor,\"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755d453-6933-46d3-8ffe-f53c13be41d2",
   "metadata": {},
   "source": [
    "In-place can save memory, but immediate loss of history will bring troubles in computing derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88155093-0f0d-4873-a3a9-821c3a497a59",
   "metadata": {},
   "source": [
    "### Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54970498-c4fd-433e-a6b7-e5fdf4be6630",
   "metadata": {},
   "source": [
    "What:\n",
    "\n",
    "Dataset: it stores the samples and their corresponding labels.\n",
    "\n",
    "DataLoader: it wraps an iterable around the Dataset to enable easy access to the samples. \n",
    "\n",
    "Why:\n",
    "\n",
    "1. Maintain code for processing data by decoupling it from model training code to provide better readability and modularity.\n",
    "2. PyTorch domain libraries provide a number of pre-loaded datasets and functions, which can be accessed by Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899cedc-e3de-4a68-b174-b6790057cbb3",
   "metadata": {},
   "source": [
    "#### Load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e28d43-ac04-4a93-a2d1-5d6dbdd5af13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# FashionMNIST Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\", # show the path where train/test data is stored\n",
    "    train = True, # specify training or test dataset\n",
    "    download = True, # download the data from the internet if it's not available at root\n",
    "    transform = ToTensor() # specify the feature and label transformations\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\", # show the path where train/test data is stored\n",
    "    train = False, # specify training or test dataset\n",
    "    download = True, # download the data from the internet if it's not available at root\n",
    "    transform = ToTensor() # specify the feature and label transformations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f748aec0-2a46-4ce1-8f26-949291a16c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img, label = training_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbb34433-1c98-4fbd-9b38-a538e49b9468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6eb4a30-ab16-4d96-bc47-7547e22a67b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.squeeze.html\n",
    "img.squeeze().shape # Returns a tensor with all the dimensions of input of size 1 removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "641c893d-dab5-4463-a140-6f6cea21097f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316fff5f-dd49-4951-acc7-d4c51c358935",
   "metadata": {},
   "source": [
    "#### visualize by map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615e23dd-6cff-48cc-ad1a-c7f4452ae67a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAHRCAYAAAABukKHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABLY0lEQVR4nO3deZycRbU38N8xZN8m+0b2jSQQQkJIAkjggnJlkYBGlojwKrKoKMgVVLyKgsJFXwWRV9R7Ba4CEr1eLotBBROBAElYspCwZN8z2TOTPYR6/+gn16lTp9KVzmSmu+f3/Xzygaqpfvrp6eqn5ulzqkqccyAiIqLQh+r7BIiIiIoVB0kiIqIIDpJEREQRHCSJiIgiOEgSERFFcJAkIiKKKPtBUkSciAw41J8R1TYRuVJEXqpRZv+joiAi00TkqsjPeonIdhFpVNfnVQxKZpDM3sQtItK0CM7lShHZn3Wc7SKyRESuq6VjPyQid9TGsejIEZFlIrIre/8rReRBEWlV3+dFDUeN6892EfmgRn/cLiKTjPbfFJGl2c9XicjjKc/jnFvhnGvlnNt/kHOJDrKlriQGSRHpA+DDAByAj9fv2fyvV7KO0wrAJwHcLSIn1PdJUZ06P3v/RwIYDeBb9Xw+ByUiR9X3OVDtOXD9yfrgCmT9Mfv3SM22InIFgMsBnJW1PxHA84d7DpJTEuNIoUrlxX0GwKsAHgJwRc0fZHde94vIMyJSLSIzRKS/dRAROVVEVorIGcbPmorIj0RkRXZn8ICINE85OefcGwDeBjCkxvE+LiLzRWRr9ldWzZ8Nyeq2Zm0+ntVfDWASgJuzv/aeSnl+ql/OudUApgA4NvsK9X8Ho9S/sEWkrYj8p4hsEJHlIvItEflQ1i+3isixNdp2yu4aOmfl80RkdtbuZREZXqPtMhG5RUTmAtjBgbLBGg3gz865xQDgnFvnnPulatNbRKZn19G/iEhHIHeTUrNfZ336+yIyHcBOAL9B7ibmZ9l162d197KOvFIaJB/J/p0tIl3Uzy8F8F0A7QAsAvB9fQARORvAYwA+4ZybajzHvwEYBGAEgAEAegD4dsrJicjo7LGvZeVB2XPdAKATgD8BeEpEmohIYwBPAfgLgM4ArgfwiIgMzjrtIwDuzv4aPD/l+al+iUhPAOcA2HIYh7kPQFsA/QCMR67P/x/n3B4Af0Sujx/wKQB/d86tF5GRAH4N4BoAHQD8AsCTKixxKYBzAVQ4594/jHOk0vUqgM+IyNdE5MRIfPEyAP8HuetSEwD/cpDjXQ7gagCtAVwJ4EUAX8quW1+q1TOvZ0U/SIrIqQB6A5jsnHsdwGLk3sya/uicm5ldAB5BbqCraSKAXwI4xzk303gOAfB5ADc65zY756oB/ADAJQc5tbHZX+7bAcxE7q+phdnPLgbwjHPur865fQB+BKA5gJMBjAXQCsBdzrm9zrm/AXga/kWQSsMTIrIVwEsA/o5cnzlk2QXrYgDfcM5VO+eWAfi/yF2IAOBR+P3jsqwOyPXbXzjnZjjn9jvnHgawB7l+dsBPnXMrnXO7Cjk/Kn3Oud8i9wf52cj11fUi8nXV7EHn3HtZP5mM8Dpa00POufnOufeza1zZKvpBErmvV//inNuYlR+F+soVwLoa/78TuUGophuQG2TnRZ6jE4AWAF7PBr6tAJ7N6mNedc5VZN/vdwUwDP+4SHYHsPxAQ+fcBwBWInd32h3AyqzugOXZz6i0TMj6QG/n3BcAFDoIdUTuL/flNepq9om/AWguImNEpDdyF6//zn7WG8BNB/pt1nd7ItfPDlhZ4HlRCZJ/ZKNuz/6IBwA45x5xzp0FoALAtQC+l33DdkC+62hNDaZPFfUgmcUEPwVgvIisE5F1AG4EcLyIHH8Ih5oIYIKI3BD5+UbkLnDDsotehXOubTYA5uWcqwTwXwAOfD26BrmL14HXIchduFZnP+upgt29sp8BueQkKk07sv+2qFHXNeFxGwHsQ40+gxp9IvuDajJyd5OXAXg6+7YDyF2svl+j31Y451o45x6rcSz2qQakRjZqK+sa5pzb55z7PYC5AI4Nj5D2NHnKZaOoB0kAEwDsBzAUub+eRyCXHPMicjGbVGsAnAngyyLyBf3D7CL0KwA/qZEM0UP9lRUlIh0AXAhgflY1GcC5InJmFoO8CbmvwF4GMAO5i+nNItJYRE5HbnD9XfbYSuTiUlRinHMbkBvYPi0ijUTkswDMJDL1uP3I9Znvi0jr7G7xqwB+W6PZo8h9JTsJ//iqFcj122uzu0wRkZYicq6ItK6ll0VlQHLT1s7N+teHRORjyH37NaOWnqJsr1vFPkhegdz35CuybKx1zrl1AH4GYNKhZOo551YgN1DeEsk2vAW5pJ9XRaQKwHMABh/kkONqfJ3xNoANyH3nD+fcuwA+jVwyxkbkBsHzsxjkXuSmsXws+9n/A/AZ59w72XH/A8DQ7KuzJ1JfHxWNzwP4GoBNyF2EXk583PXI/fG0BLkY56PIJeQAAJxzB/646o5cJu2B+tey5/wZcolDi5BLpCCqqQrAN5GbKrIVwN0ArnPOvXSwBx2CewF8UnJz2X9aS8csCsJNl4mIiGzFfidJRERUbzhIEhERRXCQJCIiiuAgSUREFMFBkoiIKOKgUyhEhKmvDZhzTurjeeuy3+XWefAVmvHdurU/NbFFixZBm1at/LndixcvTjr2iSee6JVfe+21Qz4fAKiurjZa+mrzd1KI+uh39X2tGz9+fFB30UUXeeVzzjknaPPXv/7VK8+ePTtos3KlvzjO8uXLgzYLFiwI6saMGeOV27dvH7Rp3LixVx4yZEjQ5uqrr/bK8+fPD9r8+te/DuqeeOKJoO5IOVif450kERFRBAdJIiKiCA6SREREERwkiYiIIg66LF19B7OpfpV64k5tJqBccYW/O5uVoLBvn7+t3oYNG/Ied8KECUFdu3btgrof//jHXvn998O9k08//XSv/NZbbwVtfvlLfzP6PXv25D3HulZuiTuNGvn7G993331Bm1GjRgV1zzzzjFc+44wzgjbHHXecV37hhRfyno/1nh91VJjD2axZM6+s+zcQfp6GDx8etNm2bZtXthJyTjvttKBu+/btXvnaa68N2qxfvz6oKwQTd4iIiArAQZKIiCiCgyQREVEEY5IUVeoxyRRdunQJ6saNGxfU6TihnkQNAJ06dfLKbdu2DdroeJB+DABs3bo1qFuzZk1Qp+mFAtatWxe0qaio8MrW5//VV18N6pYtW5b3+WtLKcckW7ZsGdTNmjXLK8+ZMydoYy0C0Lt3b6+8cOHCoM0ll1zilbt37x60efPNN81zrWnHjh1B3QcffOCV27RpE7QZMGCAV7b67uTJk71y586dgzZWP+zatatXPuWUU4I2I0eO9MrW60jBmCQREVEBOEgSERFFcJAkIiKK4CBJREQUwcQdiirHxB2928Kxxx4btNm9e3dQt3nzZq9sfW6aN2/ulTdu3Bi00ZO2jz766KBNZWVlUKcnTVsTu/XzW7uA7Nq1yytbCxfo5B4AmDFjhle2do3QizcUunBDKSfu3HbbbUFdhw4dvLK1EIRFT8L/0IfCe5pNmzZ5ZWs3kWHDhnlla+GAJk2aBHV6MQzdd6w2r7zyStBG73xjLWZgPb/um1Z/1n1OL/qRiok7REREBeAgSUREFMFBkoiIKIIxyQbgpptuCuqef/55r2xNZi7mmGShi5ffeOONXtmafGwdW7fTiy8DYexp586dQRs94V/HEQE7ZqVfm57oDdhxSk3HkPr27Zv3MUAYx3rssceSHleIUopJ6gW977///qDNSy+95JV1PwHC2KLF6t96UQtrAQnNWni/RYsWQZ3uY1ZMcO/evV5Zxx+tx1nHsejXZn1WdJz2kUceCdo8++yzeZ+LMUkiIqICcJAkIiKK4CBJREQUwUGSiIgoIpxV2kBZE3Wt5IjaopNDrJXxrcno+/fvP+Tn0ruXA+Gu56XGer/076Z///5BG500YO1sYO1koJ/PSrTQ/cXaKUQn5aROLNftrAnZuk81atQoaKMTcKyJ5VZfrKqq8srW7iVWQki5u/TSS73yyy+/HLTRv7tRo0YFbVI+61bijl5wwNqFRL/H1vurE3Cs50/pT82aNQva6L5rfS6shKOTTz7ZK1s74ejP6kUXXRS0SUncORjeSRIREUVwkCQiIorgIElERBTRYGOS+vt1K9ZnLT79pz/9yStv2bIlaDNmzBivrCeQW3XWLvY6lgEAvXr18sp64WkgnHRr7Wj+zjvvBHWlJCVerBd2BsIYivW+W/FOK9ajWbHMfKznt2JPup0VS9QLs1txS93vrb45dOjQoE4vjGAtDD916tSgrpxcfPHFQV23bt28so4RAuHn2Po96fgbEF5r9GcfAJo2beqVrX6h3zvrHK0+n3KN1H3O+pzohQqs+KN13u3bt/fKr7/+etAm32MA4KqrrvLK//7v/573ODXxTpKIiCiCgyQREVEEB0kiIqIIDpJEREQRJZ+4U+huECmT8m+55ZagrmvXrl7ZCjgvXbo077F1co21q4QVTNcTaq1dHPRK/FagvtSlvMfW70b3F2sxAWtid79+/bzy6tWrgzY6iUGXgfB9t/qPtQO8TtCwdnLXx7ImX+t+pnd/B4CKioqgbtmyZV7Z2smi3Fn9Qv9erN/noEGDvLK1O0yPHj2COp0UtHjx4qCNfj7rs57Sd6zFBHRdSp+zksV0cpiV4HbFFVcEdevXr/fK1o49AwcO9MqbN28O2lg7HB0K3kkSERFFcJAkIiKK4CBJREQUwUGSiIgoouQTd1ISOKzV61MSdyZNmhTUrVy5Mu/jdKDcCtTr1fKtoLiVwKHbWQk/OuBu7djQEOjEByBMpundu3fQxnq/dEKCTuACwoQEK9FAs953aycF3c5acUj3O6tv6MQh6/VbyURaz549gzrdz8ptV5Dnn38+b91nPvOZoM1pp53mla2VtHQCChC+V9bqXnpHDatf6OuIlRBoSelz+py6dOkStNH90koo+8hHPhLUTZkyxStbK6C9+OKLXvlQV9NJwTtJIiKiCA6SREREERwkiYiIIko+JpkiJf745S9/OaizVqvXrMnDeiduKzalY6n79u0L2lixAx1vtNroY1nxM71Dxvz584M2pUYvDGDF1vTv3YoJ9unTJ6jTk8atx+n3wooF68nOuq/Ejp2y64leKMHakUHHTa04T8ouEcccc0zQ5uabb/bKX/va16LnWq7+8z//M6h79tlnvfIdd9wRtHnllVeCurlz53plaxcQ3S+seLZuk3JdAYDWrVt7Zes6quPeVj/VizBYfW7evHl5z+n2228P2liLetQ23kkSERFFcJAkIiKK4CBJREQUwUGSiIgooiwTd3RgOiXp4ZprrgnqrF0crAQbTSdjpCRiWLuZWIsgWIkemp4MrwPwAPD1r3/dK19++eV5j1vshg4d6pWt90on7lg7towdOzao04tIWEkMOtHAev+svpAiJUFj06ZNXtk6R51oZu2+8MADDwR1OmlEJ5UA5ZH8dSTo3SyuvvrqoI21OMN3v/tdr/zuu+8GbfS1Ti8uAIT90rrWWMk8OvHNepzeMcbaHUb33RNPPDFoc9FFFwV1lZWVXlnvJlJXeCdJREQUwUGSiIgogoMkERFRRFnGJFMWPT/99NO9sjVRd8aMGUGd/g5ex4GAMJZp7fSuv++3ztmKaenHWZOA9fNZu3XrxdvLISbZt29fr2wtCN2xY0evrONFgB17ad++vVe23nf9HlrvX0rc0oorpcTVW7Ro4ZUXLVoUtGnbtq1XthbDsHaX1wtwz5kzJ2hzzjnneOVp06ZFz7Uh0e+xFSu2NjPQC36sWLEiaKNzFKwFNPRiJtY1w+pfKQuXpMQt9aIa/fr1C9pYfU7HIK1jp1zrDxfvJImIiCI4SBIREUVwkCQiIorgIElERBRR8ok7VuJDyq4fN910k1desGBB0MZKoNB1VjBbB9OtSbD6OFYA2jq2DvDrXb+BMAhvBcX17+jTn/500KbU6N1O9M7uQBj8t5IBrPddJ+5YyVD6/bKSKPKdD2AnUehjWX1Dv15rwQG9w4e1i4I1IVz3aZ0ABIS7fuhdQRqqlOuRlUyjk8Osz3HKrjZ6Nxi92Ahg9xV9bbGeXycFWZ85zdrxw7qOFQveSRIREUVwkCQiIorgIElERBRR8jHJFKeddlpQN3r0aK+8ZMmSoI010Vp/n28toq1jAFZsStdZx0mJTVnxzg0bNnhlKwahYwdnnHFG0KbUrF271itbC9TrGHaPHj2CNnondSDsH3riPgBUVVV5Zet918+fGotOaaOfz1rYXv9OrEWzrXijfr1Wn3rhhRfsk23gdNzZes+tz7+OH1txO12Xch1p1apV0MaKierztuKWuh9a55gST7eOXSx4J0lERBTBQZKIiCiCgyQREVEEB0kiIqKIkk/cSZmoqyc5A+GO9NYkXCvxQgemrdX7NSsorYP3VsDdem06wG4lUOiAu5XAoY9jrcxfavTrthYF0Ek5w4cPD9osX748qNNJQTo5CwC2b9/ula33XZ+T1X9SdgEpdEcGnYz2wAMPBG30DjFAmHC0cuXKoM3f//73oI7SWO+n7qt6QQcgvB5Z1wN9HCsxy+qr+ljW9VD31UIXTkjZ5aa+8E6SiIgogoMkERFRBAdJIiKiCA6SREREESWfuGO58sorvfKIESOCNjpxx0rEsJIq9Eo1VsKPDkJbQWkrCK9ZO5zo4HnKqv/W6jA64K6TTkrR0KFDvfKaNWuCNnqFIut38/TTTwd1emeMlJVyCl1Nx+ob+nFWG903rGQM3Teee+65oM11110X1OnfpbWa0fjx471y9+7d8x6HcqzkOuuapOk+Zl2zOnbs6JWtVbqsa5S+tqRcs6wdZPSxrddqrW6mWZ+dlEShw8U7SSIioggOkkRERBEcJImIiCKKJiZpfd9sxXR0nfW9/be//W2vvHXr1qCN/u7cmsxvTerW34FbbXSdFfdKmTxrvTYdJ7BiEPr1bt68OWij4wsrVqwI2owdOzbvORaTk046ySs/8cQTQRu9k7q1I8KqVauCujZt2uR9/pSdXfR7ak2stmI/Oo6TEtu0+o8+J+s4VsxI70gxcODAoM2mTZu88uDBg4M2DTEmmdIvdNwQCPvG+vXrgzb9+/f3ylZsT/d56/mt91z3Q2sxgRT6mmldj1M+X/W14ADvJImIiCI4SBIREUVwkCQiIorgIElERBRx2Ik71oT3QiZMpwZldeLKvHnzgjY6yWDPnj15j2MFpVMmdetV+IFwwQHr2DoByUrusRJuFi9e7JVTVtS3nr99+/Z5n6vUbNmyxStbv9OePXt6Zf1eAfbvy0ps0PTkfWsXjpQJ2lbCmk62sJJydN+0FhPQn1dr14j/+Z//Cer69u3rlSsqKoI2xxxzTFBHaZPwdb8EwvdTf2aBsD/pzwAQXg+sZEPrmq2vLVZ/StkVSbexrpkpuxBZn4u6wDtJIiKiCA6SREREERwkiYiIIg4ak7RiKvp7YWuB2dpadPbSSy8N6u69916vXFlZGbTRMUgr7qRfm3XOKTvLW9+v67iPNXm3V69eXlkvuA4Ab731VlCnn8/aZVzH1Kzv8nVMqRwWONfvu/V71++NFS+y4nRHH320V37vvfeCNjoGav3edb+z4kPW8+sFxa0YfkougI4zWYsp/OlPfwrqvvjFL3rlV155JWgzceJEr6wXdwCAqVOn5j3HhqhPnz5BXUp/1u+nFYfXbay+k9KfrDb6HK1rpo6bpuSIFBPeSRIREUVwkCQiIorgIElERBTBQZKIiCjioIk7KZM3rcD/mWee6ZWt3QDGjRvnlY877rigjTVh+e233/bK1sRvncyidycAwmQeK+nBWlFfB88L3c1DJ+WMHj06aDNp0qSg7q677vLKr776atBGvydWcooOlFs7DJSaD3/4w1555syZQRs92drqv9Z7mpLwk3Ic/Xu3FuOwEuY06/mtpA1Nf6atSewbN24M6vRuE9bj/va3v3nl6dOn5z2fhiDlOmotxKDf45RFAKzFRfQE/+rq6qCNdY3SSThWf9bPpxPcgHDXDyu5x3pcseCdJBERUQQHSSIioggOkkRERBGHvMD5rbfe6pWHDRsWtNHfk1uLQ+udqNeuXRu0sWIjehK+NcFWLwxgTVTVcR/rO3kr7qOPbX2/rl//c889F7S55JJLgjrNWnRcTyq3zlHHuayFEnR8w4pTlJpnnnnGK69ZsyZoo2Pf1qIAnTp1Cur079CKheuYjRVv1L93K46od5K3WLEnfWwrpq8/U127dg3aWP1OLzZh9fshQ4Z45Tlz5gRtyGbFG3X/tX7nOt/CWmBc97GOHTsGbazPfyET/K1FSXRM1rpm9+jR45Cfq67wTpKIiCiCgyQREVEEB0kiIqIIDpJEREQRB03csZJL9G4ACxYsCNroyaPWrgZWcoBmJTXoYHZKUpAV8NZ1ViKGFbjWyRDWsa+//nqv/MADDwRtUljH1osgWDuc6AQSK3FH7yZiLbhQaiZMmOCVZ8+eHbTRSTlvvvlm0GbgwIFBnZUQoekkKuv3rvuZ1X9TFnZI2e1Bfw6s57f6vfX8up2VMKaTyj7xiU8EbZ566qmgjuz3QddZn3VdZ/VT/dm2dg6y6GQe63E68ctanEP3Cytxx7rWpdAJmCkLNxwq3kkSERFFcJAkIiKK4CBJREQUwUGSiIgo4qCJO9bKH6+//rpX7ty5c9CmT58+XtlaeUQnl1jB3KqqqqBOr+hgrVShg8dWMFkHxa3j6IAzADz55JNe+ZprrgnapOwMolflsR6zevXqoG7x4sVe2UoOSVkdprKy0iunJFIVk/bt2wd1Tz/9tFe2+pROxnr33XeDNv369QvqdFKM9X7pJAqrT1mfBc1KytGfBWvFnZQ+pT8L1u/IOrb+LFi/f72zTTHv7FCfrF1erEQonThjXY/1e24l16QkXVlJXrqvWiug6aSg7t27B210P7QSIq1kMX0sawWtusA7SSIioggOkkRERBEcJImIiCIOGpO0Jv7qOus78JEjR3rlU045JWijd/Owdjrv3bt3UKcnels7HWhW3E5Pup0xY0bQ5qqrrgrqli1blvf5UqTEj6zdKHQsyHptgwcP9soLFy4M2pxwwgle2YpbFjMrJqZjLVa8TbMWw9AxdSCME1rxId0XrYnd7dq1y3tOKTuDWPGhoUOHemVrYrV+n1MncetdeqydJHTc1Oqb5c6KN+r3wcqR2LNnT1Cn+5P1OB0/TtkdZt26dUEb6zquY+zWtbZDhw5e2cpR0bFE67Va+R/WjiL1gXeSREREERwkiYiIIjhIEhERRXCQJCIiijjsbI1t27YFdVOnTj1ouTZZuyi0bNnSK1tJMTqpQk/cTWVNDq+thIUpU6YEdXoRACuRSJ+TNXlYJwFYk+qLWY8ePYI6K2lB04kqb7/9dtDmtNNOC+q6dOnila1+pxNXrKQY3e+s5BprEQJ9LL3TDhBOyLYSgHRihdU3LDpxx0oiWbFiRdKxyplOyAPC64HuS0C4Kw8Qvp/W4iL6WqfLQJiUk7IoARD2H+saqd9zKylIJ8JZfc76POnkTr1YBZC2UMLh7gzCO0kiIqIIDpJEREQRHCSJiIgiSmsGucH6nrzQ+GIhCo0/psSCdu7cGdS99NJLBT1fubHiM/r3ZcUEN2zY4JWtOOZ9990X1N15551e2VroQcfprAn/OoZiLS5gLYKgF0+wFqTXE7KtWLyeoP3zn/88aGPRvzcrrqYXrp48eXLSsRsaK55sXUf0At9W3G7Lli1e2coRseKNmtVX9LGt66qOd+o4IhAuSmAtHKD7FwCsWrXKPtmDONz4o4V3kkRERBEcJImIiCI4SBIREUVwkCQiIooo+cQdapj0juhAOAnfmpSvJ/yn+sY3vpG3jX4+a5d2PWk8dZd6nZBgJRzp34m1C0mhdLKJtbu8/t2m7MJSblIS+ayErnvvvTeo+8pXvuKVjznmmKBNyqIoevGAxYsXB22sxSH0sa1ktWHDhnllK9lQJxNZyUUPP/xwUGclOGl10cd4J0lERBTBQZKIiCiCgyQREVEEY5JUkqyJ1ToeYk0s1gvE1yYdH1m+fPkRe67aYsVErd/b0qVLvbK1u3zHjh29shUTnTNnzqGeYoOwaNGioO7666/P+zi9eLhedAIATjnlFK980kknBW2sOLie9G8tjD5r1iyvfP/990fPtVTxTpKIiCiCgyQREVEEB0kiIqIIDpJEREQRciRWTSciIioHvJMkIiKK4CBJREQUwUGSiIgogoMkERFRBAdJIiKiCA6SREREERwkiYiIIjhIEhERRXCQJCIiiuAgSVRCRMSJyICEdn2yttwOj+gwlMUgKSLba/z7QER21ShPqu/zo/InIqeKyMsisk1ENovIdBEZXd/nReWF17q6VxZ/ZTrnWh34fxFZBuAq59xzup2IHOWcC3cXrUPFcA5Uu0SkDYCnAVwHYDKAJgA+DCDcmZjoMPBaV/fK4k4yRkROF5FVInKLiKwD8KCINBWRe0RkTfbvHhFpmrW/UkReUsf436+3ROQcEVkgItUislpE/qVGu/NEZLaIbM3uKIbX+Nmy7BzmAtjBr8DKziAAcM495pzb75zb5Zz7i3Nuroj0F5G/icgmEdkoIo+ISMWBB2Z9419EZG52F/q4iDSr8fOvicjarK9+tuaTisi5IvKmiFSJyEoRua2uXjAVF17rjpyyHiQzXQG0B9AbwNUAbgUwFsAIAMcDOAnAtxKP9R8ArnHOtQZwLIC/AYCIjATwawDXAOgA4BcAnjzQITOXAjgXQEU5/HVFnvcA7BeRh0XkYyLSrsbPBMCdALoDGAKgJ4Db1OM/BeCfAfQFMBzAlQAgIv8M4F8AfATAQABnqcftAPAZABXI9a3rRGRCLb0mKj281h0BDWGQ/ADAd5xze5xzuwBMAvA959x659wGAN8FcHnisfYBGCoibZxzW5xzb2T1nwfwC+fcjOxO4mHkvmobW+OxP3XOrczOgcqIc64KwKkAHIBfAdggIk+KSBfn3CLn3F+z/rcBwI8BjFeH+Klzbo1zbjOAp5C7qAG5wfNB59xbzrkdUIOrc26ac26ec+4D59xcAI8Zx6aGg9e6I6AhDJIbnHO7a5S7A1heo7w8q0vxCQDnAFguIn8XkXFZfW8AN2VfP2wVka3I3THUPO7Kgs6eSoJz7m3n3JXOuaOR+8u7O4B7RKSziPwu+8qqCsBvAXRUD19X4/93AjgQd+oOv9/U7LcQkTEiMlVENojINgDXGsemhoPXuiOgIQySelfpNci90Qf0yuqA3NdXLQ78QES6egdybpZz7gIAnQE8gVySBpDrFN93zlXU+NfCOffYQc6DypRz7h0ADyE3WN6J3Hs/3DnXBsCnkfsKNsVa5C5AB/RSP38UwJMAejrn2gJ44BCOTeWH17ojoCEMktpjAL4lIp1EpCOAbyP31z0AzAEwTERGZMkTtx14kIg0EZFJItLWObcPQBWA/dmPfwXg2uwvexGRlllSRes6e1VUb0TkGBG5SUSOzso9kYvLvAqgNYDtALaKSA8AXzuEQ08GcKWIDBWRFgC+o37eGsBm59xuETkJwGWH+1qorPBaVwsa4iB5B4DXAMwFMA/AG1kdnHPvAfgegOcALATwknrs5QCWZV+bXYvcXQGcc68h9139zwBsAbAIWfIFNQjVAMYAmCEiO5AbHN8CcBNycaCRALYBeAbAH1MP6pybAuAe5JImFmX/rekLAL4nItXIXQAng+gfeK2rBeJcWd0ZExER1ZqGeCdJRESUhIMkERFRBAdJIiKiCA6SREREEQddV09ESjKrZ9q0aV557NixQZuFCxd65Q99KPx7YfPmzXmfa//+/UFdhw4d8j7uuOOOy9umvjnn6mXOXan2O6od9dHvSrXPfeMb3/DKGzduDNroa9tRR4WX/WbNmgV1jRo18soffPBB0KZv375eedWqVUGbO++8M6grNgfrc7yTJCIiiuAgSUREFMFBkoiIKIKDJBERUcRBV9wp1WC2DjBXV1cHbXTCjRXMtuqsBB9t7969Xrl163BZwyFDhnjld955J+9x6xoTd6g+MHHH1rFjuMHLhg0bvPK2bduCNm3bts177EJXXlu/fr1XrqioCNpYSUHFhok7REREBeAgSUREFMFBkoiIKOKgiwmUAmvivoj/9fKePXuCNjpuqeOIVhsg/O5ePxcAvP/++165efPmQZuJEyd65dtvvz1oQ0R0wMiRI/O20TFKAKisrPTK1jXLutbpvA2rjV5wwIp/6liqteBBMeOdJBERUQQHSSIioggOkkRERBEcJImIiCJKPnHH2uFD27dvX1DXtGnTvI+zJtjqOquNDozrRB4A6N+/f97nJyI6YNiwYXnbWEk5OnEwJSHRqrPa6KTIJk2aBG3OOussr/y73/0uaFPMeCdJREQUwUGSiIgogoMkERFRRIOISaZ8B2+1sepSFjjXE2x37twZtOnRo0fe4xARHWBtlKClLAqQkkdhtbOOnSIl/6OY8U6SiIgogoMkERFRBAdJIiKiCA6SREREESWfuNO7d++8bXQiDRAGpa1JsNaK9tXV1V559+7deY9t7TCSct5ERAd07949qEvZlShlUYCUHT6OOiocLrZv3+6VrYVT+vXrF9SVEt5JEhERRXCQJCIiiuAgSUREFFHyMUlror6WsnhvRUVF0Obee+8N6r7whS94ZSsGoOOU1vMTER0KvZi4pdAFUKzFTdauXeuVrY0i9OLpu3btCtpwMQEiIqIyxUGSiIgogoMkERFRBAdJIiKiiJJP3Fm0aFHeNtYE15Rg9g033BDUXXTRRV756KOPDtps27bNK1ur97/zzjt5n5+I6ID33nsvqNOJg9a1Ti8CoJNtAGDWrFlB3ahRo7zy+vXrgzY6mce6ri5fvjyoKyW8kyQiIorgIElERBTBQZKIiCiCgyQREVFEySfubNiwIW8ba/X6li1bFvR8Ogjds2fPvI+xEneWLFlS0PNTOiuJQK9IktKmrl177bVB3W9+8xuvvGPHjqCNTuIodKUnaxUpzTp2oY+jNLNnz87bplmzZkGdTuax3oNvf/vbQd3UqVPzPp9+z63dlFauXJn3OMWMd5JEREQRHCSJiIgiOEgSERFFlHxMcv78+XnbWDEm/V36ihUrkp6vqqoq7cQO8lwA8O677x7ycejQpMQWC40/WrvG6J3jrR1qBg8enPc4gwYNCuoGDhzola2Ydkrf1DFY6/UXGjdMeVxtxU0bImvCv2bF2Bs1auSVrevRmjVr8h7bepyus/I/3nzzzbzHLma8kyQiIorgIElERBTBQZKIiCiCgyQREVFEySfupEywtehgdspuItbzfexjH8v7GCvg/dZbbyU9Hx1Zn/vc54K6ysrKoG7t2rVeeeTIkUGbuXPnemUrieHmm2/2ylY/+MlPfhLU9e7d2yuPHz8+aLN06dK8x05JVOrSpUtQp3eAKCRJJ/VxZNu7d29Qt3v3bq9s9bn9+/cftAzYO4xoVlKQfr5du3YFbVavXp332MWMd5JEREQRHCSJiIgiOEgSERFFlHxM0tqJW7O+p9fxkpTJtEDtxRJTFkGg2qfjbZ07dw7aNG7cOKj70pe+5JXfeOONoI1eBGDixIlBGx2TnDx5ctDGWnx/yJAhXnnPnj1Bm3POOccrd+jQIWjzwgsveOXRo0cHbSZNmhTU6cULNm/eHLQ55ZRT8ra59dZbvTJjlIdHb7jQrl27oI2+Rm7fvr2g50pZ1L4cF0nhnSQREVEEB0kiIqIIDpJEREQRHCSJiIgiSj5xx6IDzCk7putJuTF6EYJC26TuOkKF69q1a1D3yCOPeGUricFKztKJK2PGjAna9O/f3yvffvvtQZtVq1bZJ1vDjh07gropU6Z45fPPPz9os2/fPq980kknBW3GjRvnla3EJet3MmrUKK9s7UKiE3X0AgwAE3Vq2+LFi73yySefHLTR/alJkyYFPZe1mICuK8frGu8kiYiIIjhIEhERRXCQJCIiiijLmOTGjRu9srWYQCFxS4u1WHDTpk298rp16wo6dkNlTebX8bY2bdoEbe655x6vbE3mb9WqlVd+6qmngjbnnXdeUHfHHXd4Zb2YORDG6awFI1IWe7ZiP3ph8meffTbvOb7zzjtBG/17tD4b1jn+/ve/98oLFy4M2lixVDqy9KIWp556atBGX9t0H4hJWXRA51+88sorSccuJbyTJCIiiuAgSUREFMFBkoiIKIKDJBERUURZJu7oleiPP/74oI1O3ElZAAAAWrdunfdxOvHC2q2b4qzEgmOOOcYrP/jgg0GbESNGeOUFCxYEbTZt2uSVreSeadOmBXWf//znvfJHP/rRoM21117rlYcOHRq0+dWvfuWVrWSXlMSdbt26BW1037R+jzrRrHv37kEb63dbSMKRPmcgTCLh4gKHZ+bMmV455feZsnMSECZANm/ePGij+5jelaQc8E6SiIgogoMkERFRBAdJIiKiCA6SREREEWWZuKOTaaxgtk4qWL9+fdKxrdVg8rFW5aFDo5MIrNVsqqurvfIzzzwTtNHJPXfffXfQ5itf+UpQ99WvftUrn3XWWUGbJ554witbu4Do1Xwef/zxoE1Kfxk+fHhQt3XrVq9sJVrox1mJQ1/84heDum9+85t5zyklaYSJO7VLJ1SlJEtZ/cKSssqVbtOiRYukY5cS3kkSERFFcJAkIiKK4CBJREQUUZYxyaqqKq9s7fChdz/Q8ZyYlBX0dUzU2jGB4qz3S8ckb7zxxqDNmDFjDloGgL/85S9e+YILLgjaWLu26NjhsGHDgjbPPfecV9YTva1z0nFMANizZ09Qp1mLWOi4+ttvvx20mTFjhle24lN6pxQAGDx4sFfWC3YAafFFK2ZGhdOfC2uhAL3Iw86dO5OOrd9Pa5GLfOdTDngnSUREFMFBkoiIKIKDJBERUQQHSSIiooiyTNzRk7GtRBCdQDB79uykYy9evNgrW0kWOqnCen6KS0kA0QsHAGHijC4DwMUXX+yVu3TpErSpqKgI6v785z975e985ztBGz1RXyfJAMCJJ57ola0EpLvuuiuo0y666KKg7tRTT/XK06dPD9r8/ve/98qXXXZZ0GbUqFFBXdu2bb2ylfCkk+GaNWsWtNFJQW+88UbQhtLpxSCsxB39vmzbti3p2Poz1qlTp6CN/qxai1OUOt5JEhERRXCQJCIiiuAgSUREFFGWMUm9EK81CVbHCfWO9TH6O3drUreOd6YuVEA5Q4cODep0nMyKBffo0cMr68WfAeDCCy/0ylYMx1oo4Oyzz/bKVkxw0KBBXnnRokVBmw4dOnjlIUOGBG2sWObKlSu9srXAu17owoq3jhs3zitbCw6sWrUqqFuxYoVX7t27d9BG/970awXKcwHs+qSvNVb+g77+rV27NunYemGAAQMGBG10TLJly5ZJxy4lvJMkIiKK4CBJREQUwUGSiIgogoMkERFRRFkm7uhV7lN2HkhZ4R4Adu3a5ZWtXeR18FwnXdDBWYkFOomgadOmQRudIGUlrjz77LNe2Zr83LFjx6BOT8hOmfBv9Sk9eV73J8DeveTDH/6wV967d2/QZt68eV7ZSsrRyU1NmjQJ2ljH7tatm1e2kjiWLl160DIQvo96VxY6NFbioKavR7t370469oYNG/I+V+PGjb2ylaxV6ngnSUREFMFBkoiIKIKDJBERUURZxiTXrFnjlXU8CQgnkafumJ7STj9f6uRdytmyZUtQN2XKlFo5to7PWLFNa6ECvcC3FdfR8RmLnvDfvHnzoM2LL76Y9zjWYtMpi2jo3621k7wVk9SxU2sRBqp7Kdcx3Q9S3zvdN6yYpK6z+nOp450kERFRBAdJIiKiCA6SREREERwkiYiIIsoycUcHnFMWCrCSe1JYgXJ9rHLcrbtU6V0LUidWp+zkknqsw30MkL5rDZU33S+tpCu9YIS1U4ilsrLSK1sLp2ibN29OOnYp4Z0kERFRBAdJIiKiCA6SREREERwkiYiIIsoycUcHs3WyBhCuFJGauKMD49bqFTpQrldZISI6EqzEHX2ts1ZZssyaNcsrp+x4tG3btqRjlxLeSRIREUVwkCQiIorgIElERBRRljFJvQuItZiAjhOmToLVCwOkfE9f6IRxIqJDYcUk9fXvvffeSzrWzJkzvbKVW6HzPaqrq5OOXUp4J0lERBTBQZKIiCiCgyQREVEEB0kiIqKIskzcWblypVfWk2kBYNeuXV55/fr1ScfWCT7WDh8VFRVe2QqmExHVNmu3mj59+njlpUuXJh1LJ+FY17oWLVrkff5SxztJIiKiCA6SREREERwkiYiIIsoyJvnuu+96ZWsyf6ExSf09vRVv1MeeM2dO0rGJiA5HVVVVUKcn/K9du7agY1sLBTRt2tQrr1u3rqBjFzPeSRIREUVwkCQiIorgIElERBTBQZKIiCiiLBN39GICH3zwQdDG2hmkEI0bN87bZtGiRbXyXEREB2Nd11q1anXQ8uHQiwns3Lmz1o5dLHgnSUREFMFBkoiIKIKDJBERUURZxiS1m2+++Ygd+8477wzq2rRpc8Sej4go5vHHHw/q9CIA06dPr7Vj9+3b1ytbC7eUOt5JEhERRXCQJCIiiuAgSUREFMFBkoiIKEL0CvFERESUwztJIiKiCA6SREREERwkiYiIIjhIEhERRXCQJCIiiuAgSUREFMFBkoiIKIKDJBERUQQHSSIioggOkkS1TESciAw41J8RUfEp60FSRJaJyC4RqRaRrSLysohcKyJl/bqpdojINBHZIiJNi+BcrhSR/SKyPfu3RESuq6VjPyQid9TGsah4iMhlIvJa1l/WisgUETn1MI85TUSuqq1zLAUNYbA43znXGkBvAHcBuAXAf1gNRaRRXZ4YFS8R6QPgwwAcgI/X79n8r1ecc62cc60AfBLA3SJyQn2fFBUfEfkqgHsA/ABAFwC9APw/ABfU42mVpIYwSAIAnHPbnHNPArgYwBUicmz2F/TPReRPIrIDwBki0l1E/ktENojIUhH58oFjiMhJ2V9mVSJSKSI/zuqbichvRWRTdsc6S0S61NNLpdrxGQCvAngIwBU1f5D1m/tF5JnsW4oZItLfOoiInCoiK0XkDONnTUXkRyKyIutPD4hI85STc869AeBtAENqHO/jIjI/64PTRKTmz4ZkdVuzNh/P6q8GMAnAzdkdx1Mpz0/FS0TaAvgegC865/7onNvhnNvnnHvKOfe1rN/dIyJrsn/3HPi2RETaicjT2fVvS/b/R2c/+z5yfzj+LOsrP6u/V1mHnHNl+w/AMgBnGfUrAFyH3AVwG4BTkPuDoQWA1wF8G0ATAP0ALAFwdva4VwBcnv1/KwBjs/+/BsBT2eMbARgFoE19v37+O6y+swjAF7L3ch+ALjV+9hCAzQBOAnAUgEcA/K7Gzx2AAQDOBrASwEn6Z9n/3wPgSQDtAbTO+tCdkfO5EsBLNcqjAWwFMCgrDwKwA8BHADQGcHP2Gppk5UUAvpmV/wlANYDBNV7PHfX9O+e/Wuu7/wzgfQBHRX7+PeT+AOwMoBOAlwHcnv2sA4BPZNey1gB+D+CJGo+dBuCq+n6NdfmvwdxJKmuQuzABwP8456Y75z4AcByATs657znn9jrnlgD4FYBLsrb7AAwQkY7Oue3OuVdr1HdA7uK33zn3unOuqg5fD9WiLG7TG8Bk59zrABYDuEw1+6NzbqZz7n3kBskR6ucTAfwSwDnOuZnGcwiAzwO40Tm32TlXjdxXY5fotjWMze4EtwOYCeA3ABZmP7sYwDPOub865/YB+BGA5gBOBjAWuT/q7sr69d8APA3g0oRfB5WeDgA2Zn3TMgnA95xz651zGwB8F8DlAOCc2+Sc+y/n3M6sT34fwPg6Oesi1VAHyR7I3QkAub/0D+gNoHt2IdoqIluR++v7wFenn0PuL/Z3sq9Uz8vqfwPgzwB+l319cbeIND7ir4KOlCsA/MU5tzErPwr1lSuAdTX+fydyg1BNNyA3yM6LPEcnZN9c1Ohrz2b1Ma865ypcLibZFcAw5AZWAOgOYPmBhtkffSuR6+vdAazM6g5Ynv2Mys8mAB1F5KjIz72+kv1/dwAQkRYi8gsRWS4iVQBeAFDRkPM1GtwgKSKjkbs4vJRV1dx1eiWApdmF6MC/1s65cwDAObfQOXcpcl9T/BuAP4hIS5f7vv+7zrmhyP3lfh5yMS0qMVlM8FMAxovIOhFZB+BGAMeLyPGHcKiJACaIyA2Rn28EsAvAsBp9rW02AOblnKsE8F8Azs+q1iD3R96B1yEAegJYnf2sp8rq7pX9DPA/A1T6XgGwG8CEyM+9voJcX1iT/f9NAAYDGOOcawPgtKxesv82uL7SYAZJEWmT3fn9DsBvI3/hzwRQJSK3iEhzEWmUJfiMzo7xaRHplP1FvjV7zH4ROUNEjsv+2qpC7uvX/Uf+VdERMAG5924ocl+hjkAuOeZFHNofPmsAnAngyyLyBf3DrA/9CsBPRKQzAIhIDxE5O+XgItIBwIUA5mdVkwGcKyJnZt9i3ARgD3LxphnIxStvFpHGInI6coPr77LHViIXf6cy4Jzbhlxexf0iMiG7O2wsIh8TkbsBPAbgWyLSSUQ6Zm1/mz28NXJ/vG0VkfYAvqMO3/D6Sn0HRY/kP+QSd3Yhl6SwDbm/sL4IoFH284egEhaQ+9rhMeS+TtuCXID7rOxnvwWwHsB25C5OE7L6SwG8i9yFqBLATxEJmvNfcf9D7ivP/2vUfyrrE0fpfgPgdACrapRrJuf0Re7rrKuMnzVD7uvSJcj9cfU2gC9HzutK5Abv7dm/9Vk/7VyjzYUAFmR9/e/I3aUe+NmwrG5b1ubCGj8bCGA2cn/4PVHf7wH/1VpfngTgtey6tA7AM8h909Usu0atzf79FECz7DHdkUvO2Q7gPeSSEt2B6xmAcVn9FgA/re/XWBf/JHvhREREpDSYr1uJiIgOFQdJIiKiCA6SREREERwkiYiIIjhIEhERRcRWZACQ2/uurk6kUJ06hQuUnHjiiV65c+fOQZuHH374iJ3TJz/5Sa+8bdu2oM20adO88r59+47Y+RTKOSf5W9W+Uuh3DU1ubYJ/aNo03D1s9+7dtfJc9dHv2Odqh+4nAFAKMygO1ud4J0lERBTBQZKIiCiCgyQREVEEB0kiIqKIgy5LV4zB7EGDBnnlESNGBG2WLFnila+66qqgTcuWLb1ynz59gjannnpqUPf000975b179wZtmjRp4pXvuuuuoE3Hjh298syZwZaDWLt2bVBXl5i4Q4eicWN/dzjr2qLr9u8P9wEo98Sdo44K8yXffz+29eOhmTt3blCnf8dr1qwJ2lRUVAR1bdq08cqTJk1Ker589PURsK+jdYmJO0RERAXgIElERBTBQZKIiCii5GKSn/rUp7yy9f369u3bvfLq1auDNl26dPHK48aNC9pYixCsX7/eK1vfyS9cuNArt2/fPmjTs2dPr2zFBP77v/87qKtLjElSfSi3mOSHPuTfi3zwwQcFHedb3/pWUHfxxRd7ZSve161bt7zHthY80ddRnccBAK+++qpXvvHGG4M21vU3hV6Y4EguSsCYJBERUQE4SBIREUVwkCQiIorgIElERBRR1Ik71g4fZ5xxhlfWiTRAODHVmqi7detWr7xx48agTVVVVVCndz/QiwIAYRKOtTK+nqjbo0ePoM1TTz0V1Olg+pHExJ0jz5pYrhM9jqTUSewpySY6aWPq1KlBm9mzZ3vlRo0aWedUVok7Ke67776gbsKECV7Z2nlFX7d27doVtGnevLlXfv3114M2vXv3Dur0datt27ZBG31O1nhSWVnplR988MGgzUMPPZT3+S21lczDxB0iIqICcJAkIiKK4CBJREQUEQZEikjr1q2DOv0d9L59+4I2O3bsyNtGf0/fq1evvM8FpE1w1QsKW/GcnTt3HvQxANChQ4egri5jknTk1dbC1sVAx7Uee+yxoM2QIUO8cqET60tJu3btvPKsWbOCNikLl1RXVwdt9uzZ45WtxVX0te60004L2liLoqxYscIrjxo1KmijFyHQzwUA3bt398o/+tGPgjYLFiwI6vSmD/W1MDrvJImIiCI4SBIREUVwkCQiIorgIElERBRR1Ik7LVq0COp08Lpv375BGz3BdtmyZUEbPYnbSpyxJrPqybNW4oUOplsJODpRyEoushKXqHgVsmvBMcccE9RZuzakJIPpRQispAb9OGvhAuu8Gzdu7JX1YhwAMG/ePK98wgknBG1SnqvcnH766V7ZWhRgyZIlQZ1ecMRaeEEnBVnXMb0oypw5c4I21iIEeqci67z1Dke7d+8O2qxbt84rWwu3XH/99UHd5Zdf7pXrIknHwjtJIiKiCA6SREREERwkiYiIIoo6Jqm/kwfCGMZHP/rRoM0bb7zhlfXEfSCME1oTda3v91POcf78+V751ltvDdroWOYPfvCDoI3+vp+KWyExyccffzyoGzRoUFBXyKIDhS6UnrKwtPXZ0DGzVatWFfT85aZr165eecuWLUEbnccAhP3HWgxfLx6uJ+4D4fuwcuXKoI11HdPHatasWdBGxzutRdB1TNJ6/WPHjg3qigXvJImIiCI4SBIREUVwkCQiIorgIElERBRR1Ik7VnKATmqwkgyee+45r1xRURG02bRpk1e2JuqmsCbPtmzZ0is/+uijQZvzzjvPKw8ePDhoo8+RilshE+OtidVWwpZO3LH6q14oQC8AAKQl81ifKX1s67Op+32hn6lyc+yxx3pl632xErP0rhdW4oy+RliLEuh+mbrjkU7K6dSpU9BG91VrUQLdxkqkXL16dVCnd0bRu6LUFd5JEhERRXCQJCIiiuAgSUREFMFBkoiIKKKoE3esVef1rh9WwFcHwa2VKnTw3Apcp6wqYq1MrwPOzz//fNBmwIABXtnaDeKll14K6qh46aSYlBWbrCQdK+FF908r+UPvJGP1e52UYyXpWMk9+rVYbfRnodAVf8qNTsqzdjeyknKaN2/ulXUiDRC+L2vXrg3a6MRFvSIZYK+Uo69RVpLitm3bvHKrVq2CNjqZx+rfOukLAIYOHeqVmbhDRERUZDhIEhERRXCQJCIiiijqmKT1PXWPHj288h/+8IegjY6FWN/3p8SLrO/OdQzHaqNjqVab1157zStffPHFQRsdk7CObe0eQPVDT7hP0bp166AuJRZuxRJ1nNJqo2ObViw+tU7TMdGUnSXWrFmT97il7rjjjvPKVmwvJW/ij3/8Y9BmzJgxXtlalETnaJx//vlBG2uCv+4/1u4h+jr29a9/PWjTpUsXrzxnzpygjXWNHjFihFeeNm1a0KYu8E6SiIgogoMkERFRBAdJIiKiCA6SREREEUWduKN3/ADCZBZrwr2ezG8lMGgpbVIfp5MsOnToELTRwevPfe5zQRtrYq5OZmLiTvHQ77u1s4OetN27d++gjZXMYi0eoOmkrpRFAaxkI6tO9/OUxDcrKaljx45euSEk7uikGKtfWNcI/f4NHDgwaKOPZS3Aot9PazEDa8chfexu3boFbS677DKvrJO3rHOyknSs/j18+PCgrj7wTpKIiCiCgyQREVEEB0kiIqKIoolJWvE367tzbcuWLUGdnrxqfd+tJ0cXGpNMYcUJNmzY4JWtybxWDELvVk7FIyVOd84553jljRs3Jh07ZaECHUOyFrEoVMrnQ/dzvfg1AEyaNMkrz5079/BOrAToPAr92Qfsa52O740fPz5oo2OJlZWVec9nyZIlQd327duDOn0dtRZB14sg6MXMgfCaZS2SUl1dHdSNHDkyqKsPvJMkIiKK4CBJREQUwUGSiIgogoMkERFRRFEn7lgTU3WA2dppQE9WtZIOdMDZmuBr0ceydkfQx7J2M9E7AeggOQAsXLgwqLN2C6DikNI39G7rVt+0Er1SEmf0whJWX9HJPCkLBwDhZ9FKINNJG9bn9/rrr/fKt9xyS9CmlFmfY/07t94X633Yu3evV7YSofR7ZfWdzZs3e+Xjjz8+aGMlkOmkoKVLlwZtBgwY4JU7deoUtLn77ru98mc/+9mgzTvvvBPUFUuSIu8kiYiIIjhIEhERRXCQJCIiiiiaAJf1Pb21QLOOu1hxDz1ZNTXemEKfU8oE8pTv1q3JvNZkcMYki4MVt0uZ8P+JT3zCK1t9POU4Vt/Q/UzHtCxWf7Jem66zFujQMSyrT/ft29crH3vssXnPsZRYC9br2LT1nk+ZMiWoO/fcc72yji0CwIIFC7yy9fvU/WDr1q1BGyveqXM7rMXodSxTv78A8POf/9wrX3nllUEb6xppLTpQH3gnSUREFMFBkoiIKIKDJBERUQQHSSIiooiizgKxknLWrVvnla1dQPQCAzrZBwgTEQqduGolWehEoZTdTBYvXhzUWTuDWEF/qntW4ox+362kmO7du3tlq/9aCQu6D1uTxjVrB/hCd7/Rr81aKGH9+vVeuWfPnnmPe8UVVyQ9f6lo165dUKcn2FvXjD/84Q9Bnd71w0rc0e+xdc3Q159ly5YFbazkxv79+3tlq89Z12ht+fLlXtlalCDlGtmhQ4egTieLHQm84hIREUVwkCQiIorgIElERBTBQZKIiCiiaBJ3unbtGtTpHRMAoGPHjnmPpRMfrMQdzUpgsJITNCuRRj/OCnjrpA4dJAfsFVPat2/vlZcsWZL3HKl+XHDBBUGd3v1F75wB2AkKOtnDWulJ9xdrZx39OOs4VhKHTqx46623gjY6KcmiV+E54YQT8j6mlKRcn6ykL90vgDCZ0EroWrVqlVe2ksWqqqq88ogRI4I2OrnGOicrudFKJspn5syZQZ1eXQgIk4Ks1XyYuENERFSPOEgSERFFcJAkIiKKKJqY5HvvvRfUPfjgg0GdXiigR48eQRsdv7HijToukLKbh8WKL2jW83fr1s0r//CHPwzaWDFRKxZEdS9lZ5mJEycGdTpmZO2sYMWeKioqvLLVX3XMKGW3e6tN27Ztg7oVK1Z45ZYtWwZt9Gdzx44dQRsd27Tir6Vs8ODBQV3KLhwXXnhhUKdzKazryLhx47yy3pUDCOPA1g4uKYupWItDpOREjBw50itbC6ek5H9YeRuvvfZa3scdLt5JEhERRXCQJCIiiuAgSUREFMFBkoiIKKJoEnf0hFcAePnll/M+7vjjj8/bxkquSQkUp+yQYCU+6DrruXTCwvTp0/M+F9Uf3RdS+s9ZZ50V1Om+MWjQoKDNokWLgjq9MIBeVAIIF62wdlbQbaw+rnfzsFhJFDr5w0oQ0Z9zK4nDmuxeKvr06RPUVVZWemUrScbamSNlh4/Vq1d7ZSsBcvjw4dapHvS5gDDhyJq4b13/NN3Hrev69ddfn/c41oIzdYF3kkRERBEcJImIiCI4SBIREUUUTUzSkrLoeMoO7Sms79ZTFgpIYb2OlMm7KYunp8TGqG7069fPK1sT5XXMyFqE3Fr0W/fPDRs2BG10vM9a8EAvAmAtxmFtCGDFQDU9sdxa/Fof28pFKGXW+7l27Vqv3KtXr6DNG2+8EdTpGOTUqVODNnrR72HDhgVtOnfubJ9sDXrBASDsK9YCFro/V1dXB210/PrPf/5z0GbAgAFB3bx587yytchGXeCdJBERUQQHSSIioggOkkRERBEcJImIiCKKOnEnJSnF2ok7ZRd3nZRjTXwuVMp5p+w6kjJRl4rHVVdd5ZWtfqATV6yEGGuXep2gZiVj6D5s9TE9+VwnRwDA6NGjgzqdaPbXv/41aKMne3fq1ClooxNErESPUmZdRzp27OiVrWTDo48+OqjTfcNKstKLqViJULt27fLKVpJOysIP1kIJOrlI9y8AOPnkk73yL37xi6CNxUogqw+8kyQiIorgIElERBTBQZKIiCiiqGOSKayYZEq8T0/UT1nMnBoGaxGHlPjwZz/7Wa+8fPnyoE2bNm28shV/tOJDemEAq7/u27fPK1sxUR37Gjp0aNDGWgTgK1/5ile+8847857j1q1bgzb6s2nFx8qNXjzceu/OP//8oE7H+3RsEwBmzZrllUeNGhW00c+Xmuug49BWvFMvom8twq77XMr1GQj7U7t27ZIeV9t4J0lERBTBQZKIiCiCgyQREVEEB0kiIqKIkk/cqa2dOgpN1kjBnTrqhk540YksqVLe9/nz5+d9/m3btgVtWrdu7ZWtxDOrTrNem56kvmPHjqCNThxasGBB0MbaSUJ74IEHgrp3333XK1ufqUKTSEqZTlSxrgcnnnhiUKd3D7EWntDv56pVq4I2ehGA1D6n3z8r4UYnXlmv7cwzz/TK//RP/xS0sRa10OekE6DqCu8kiYiIIjhIEhERRXCQJCIiiuAgSUREFFHyiTvWivo6GSAlKUevLhGjH2cFqnUyUeqxKc56D7VCE3U0vboMAPzwhz/0yno1EABYuXKlV27VqlXe57ISVwrdtUaviGKt0PLEE0945QsvvDDvOQJA27Zt87bR5229H3qFFmvFoVJmrTijk5X0rhwAsGbNmqBOJ/O89dZbQRud3NK/f/+gzQUXXOCV58yZE7Sx3ofJkyd75YkTJwZtUhKvtmzZ4pUXLlwYtLESMPU51eZOTYeCd5JEREQRHCSJiIgiOEgSERFFlHxM0vouW8cJrVX3U2JcKQsVWN/J62NbsZnaWgShoUiJfQwYMMArW7HFk046ySsPGTIkaKMn/APAokWLvLL1nuq4ndXHdAzJ6gfWa9V92to9o1OnTl7ZmqCdGoPU9GtJiaVa8Xr9OCu2W8qseKP+PVix6rPOOiuo04tRTJgwIWijFwro0qVL0GbGjBnWqeZ12223eWVrV5cpU6Z45ddeey1oo2P1loEDBwZ1L774oldu2bJl3uMcCbyTJCIiiuAgSUREFMFBkoiIKIKDJBERUUTJJ+5Yq9frBIKUJBkruSdl946UhQqsyeH1FYQuF//6r/8a1F199dVe2Zogres2b94ctKmsrAzq9KIV1o4Eui9YfVP3xZTkFgDYs2ePV7Z2hNBJMGPGjAnaaNY5Wsk0ul3K5yVloY1yS9yxErpSFhOwdozR9EIQR5reIUZ/vmqTtVBCRUWFV16/fv0Re/6D4Z0kERFRBAdJIiKiCA6SREREESUfk7Rigjp+Y8UkUxbLteIuOhZlxb10/Mo6R2vCOsWNGDHCK99www1BGz3Z2VpsWr/vVkwsJYZttdH9LiXelrKoBRDGsK0J6XpieQorXp5i/vz5QZ1+vSkLtZfbAufV1dVBnY5TWnFLa6MG3Z+sjRJSYtx79+71yikLcwDhZ8V6fv2eW30+pY9Z12NdZ8Vy6wLvJImIiCI4SBIREUVwkCQiIorgIElERBRR8ok7eqdzIAyMWwFnHQS2EjGaN2+e99jW43QbKyjfpk2boI7i2rVr55Wt3Qb0DggdOnQI2hx99NF5n8tKftAJCTqpAggTDayJ+pqVRJGSVGYtFLB8+fK8j9OsxCEr0aKQRDcrOU0nHOmdU0qdlbiiryNW/7L6k2a9LzoppzalJBxpqYlomtUP9DXautbXBd5JEhERRXCQJCIiiuAgSUREFFHyMcmUCctWDEB/v221SVmg2oop6diM1Wb16tVBHcVNnTr1oGUgXOihc+fOQZt+/fp55U6dOgVtevXqFdR169Yt77GtGKim+4bVx5577rmg7u6778577EKkxiRT+rTegT4ltvnyyy8Hba677jr7ZEvA9OnTg7rx48d75aqqqro6nZIxe/bsoC6lr9QF3kkSERFFcJAkIiKK4CBJREQUwUGSiIgoQqyEFSIiIuKdJBERURQHSSIioggOkkRERBEcJImIiCI4SBIREUVwkCQiIor4/7tuMmVaxu1fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0:\"T-Shirt\",\n",
    "    1:\"Trouser\",\n",
    "    2:\"Pullover\",\n",
    "    3:\"Dress\",\n",
    "    4:\"Coat\",\n",
    "    5:\"Sandal\",\n",
    "    6:\"Shirt\",\n",
    "    7:\"Sneaker\",\n",
    "    8:\"Bag\",\n",
    "    9:\"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols*rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item() # generate a sample index ranging from [0, len(training_data)) randomly.\n",
    "    img, label = training_data[sample_idx] # select a random training sample based on id above.\n",
    "    figure.add_subplot(rows, cols, i) # i -> for-loop starts from index 1.\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.squeeze(), cmap='gray') # channel one is unneccessary when it comes to displaying.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c7154-fac0-4f1a-b6d1-e6638572c78d",
   "metadata": {},
   "source": [
    "#### Create a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c65d86-0966-4789-93ca-3bb52e3024fe",
   "metadata": {},
   "source": [
    "Dataset class must implement three functions: __init__, __len__, and __getitem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70d35c6-989c-43db-b38d-79e2eb51fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# by torch.util.data.Dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    # initialize a dataset object with directory containing the images, the annotations file, and both transforms.\n",
    "    def __init__(self, annotations_file, img_dir, transform = None, target_transform=None):\n",
    "        # csv files contains annotations seperated by ',' \n",
    "        # like \n",
    "        # tshirt1.jpg, 0\n",
    "        # tshirt2.jpg, 0\n",
    "        # ...\n",
    "        # ankleboot99.jpg, 9\n",
    "        # the first is image's annotation, and the second is its corresponding label idx.\n",
    "        self.img_labels = pd.read_csv(annotations_file) \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    # the __len__ function returns the number of samples in dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    # the __getitem__ function loads and returns a sample from the dataset at the given index idx.\n",
    "    def __getitem__(self, idx):\n",
    "        # locate a image by joining the file' annotation and its parent.\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # torchvision.io.read_image converts the image to a tensor.\n",
    "        image = read_image(img_path)\n",
    "        # retrieve the image's corresponding label.\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        # if applicable, call the transform functions on the image and the label.\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd470f34-ceee-4c42-a1c4-0f2db2ccc1af",
   "metadata": {},
   "source": [
    "* Typically, when training a model, we want to pass samples in \"minibatches\", reshuffle the data at every epoch to reduce model overfitting, and \n",
    "  use Python's multiprocessing to speed up data retrieval.\n",
    "\n",
    "* Compared with torch.util.dataDataset retrieving dataset's features and labels one sample at a time, torch.util.data.DataLoader is an iterable that provides an easy-to-use\n",
    "  \"minibatches\" way and abstracts this complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f240c8a9-481b-405b-8bd0-bb65b5f9c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Each iteration returns a batch of train_features and train_labels\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8280ea4-c2d6-4476-aae5-d2339691ebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAS7ElEQVR4nO3df2xVZZoH8O8jID+FUqBQKshvUAzKQnATFdlM1qAk4iRIRhODiVkMkTijo1l1/4AYjWSzM+P8YSbW1cgss05MZoiamHWIGUMmBAURBMUFt1SKFIr8kPJTCs/+0cOkYs/z1Hvuvefg8/0kpO399r337Wkfzr33Pe/7iqqCiH78rsi7A0RUHSx2oiBY7ERBsNiJgmCxEwXRu5oPJiJ8678CevdO/zV2dHRU9LH79OlTcn7q1Klyd4cAqKp0d3umYheR+QB+C6AXgP9U1VVZ7q/IRLo9fj1S6eHNmpqa1Ozrr7+u6GPX1dWZeUNDQ2r24Ycflrs732H9zrzfiff7vhyHrEt+Gi8ivQC8COAOANcBuFdEritXx4iovLK8Zp8D4AtVbVLVbwH8EcDC8nSLiMotS7E3AGjp8vW+5LbvEJGlIrJZRDZneCwiyijLa/buXtR874WMqjYCaAT4Bh1RnrKc2fcBGNPl66sB7M/WHSKqlCzFvgnAZBEZLyJXAvgZgLfK0y0iKreSn8araoeILAfwLjqH3l5V1U/L1rOCyXOoZeXKlWb+0EMPpWZHjhwx29bW1pp5//79zfzjjz828zFjxqRma9asMdt6P7fnchweq6RM4+yq+g6Ad8rUFyKqIF4uSxQEi50oCBY7URAsdqIgWOxEQbDYiYKQao5F5nm5bCWnLM6ZM8fMV6xYYeZjx4418yFDhpi5NZY+ceJEs+1LL71k5iNGjDDzadOmmbk1n33UqFFm26NHj5p5U1OTmT/33HOp2caNG822l7O0+ew8sxMFwWInCoLFThQEi50oCBY7URAsdqIgqrqUdJ6yDjFaw2cPP/yw2ba9vd3MvSWVDx06ZObDhg1LzQYOHGi2bWlpMfMBAwaY+ejRo83c+tkPHjxotvXMmjXLzN9+++3U7L777jPbrlu3rqQ+FRnP7ERBsNiJgmCxEwXBYicKgsVOFASLnSgIFjtREGGmuGa1bdu21OyKK+z/M0+fPm3mvXr1MnNvp9RVq9I3z7311lvNtosXLzbzb775xsy9awCscfZz586Zba+7zt4n9NFHHzXzBQsWpGaTJk0y286YMcPMi4xTXImCY7ETBcFiJwqCxU4UBIudKAgWO1EQLHaiIMLMZ/c0NDSYubWcszcW3bdvXzP/9ttvzdwzbty41My7jmLXrl1m7l0j4N2/dQ2Bd32BN9/9jjvuMPPhw4enZjU1NWZbb3nvvXv3mnkRZSp2EWkG0A7gPIAOVZ1djk4RUfmV48z+T6r6dRnuh4gqiK/ZiYLIWuwK4C8i8pGILO3uG0RkqYhsFpHNGR+LiDLI+jT+ZlXdLyJ1ANaJyOequr7rN6hqI4BG4PKeCEN0uct0ZlfV/cnHNgBrAdg7HBJRbkoudhEZKCJXXfwcwO0AdpSrY0RUXlmexo8EsDbZCrk3gP9W1f8pS69ycO2115q5tfVw1u2gvfnwhw8fNvMlS5akZt44+fHjx83cu0agtrbWzK05697P5c2Vv+mmm8z85MmTqZn3O/PWpA81zq6qTQBuKGNfiKiCOPRGFASLnSgIFjtRECx2oiBY7ERBcIprYubMmWbeu3f6oerXr5/Z1pvK6W3p7A3dNTc3l/zY58+fN/Px48eb+bx588x87ty5qdlTTz1ltvWGDU+cOGHm+/fvT82mTJlitrX6DQBr16418yLimZ0oCBY7URAsdqIgWOxEQbDYiYJgsRMFwWInCoLj7InJkyebeUdHR2pmTX8FgD179pi5NxY+aNAgM7fG4b1x9AsXLph5W1ubmT///PNmftVVV5V8397PffToUTO/5557UrPPPvvMbOtdX3A54pmdKAgWO1EQLHaiIFjsREGw2ImCYLETBcFiJwpCvLnSZX2wAu8Is337djOvr69Pzc6ePWu2PXLkiJl77b1xfGvLZ2/JZO++vb+PgQMHmrk1zu9tVW1d2wAAN9xgL25sbUftrUHgXX8wceJEM8+Tqnb7S+eZnSgIFjtRECx2oiBY7ERBsNiJgmCxEwXBYicKgvPZE8eOHTPzM2fOpGbjxo0z29bU1Ji5N97szdu+5pprUjNry2QAOHDggJl7c+29tdutcfb+/fubbb0xfO/6BGu7ae+Yjx071swvR+6ZXUReFZE2EdnR5bZaEVknIruTj0Mr200iyqonT+NfAzD/ktueBPCeqk4G8F7yNREVmFvsqroewKXXey4EsDr5fDWAu8vbLSIqt1Jfs49U1VYAUNVWEalL+0YRWQpgaYmPQ0RlUvE36FS1EUAjUOyJMEQ/dqUOvR0UkXoASD7ay4QSUe5KLfa3ACxJPl8C4M3ydIeIKsV9Gi8irwOYB2C4iOwDsALAKgBviMiDAPYCSF+guyCGDBli5k1NTWZ+xRXp/y9686pPnjxp5t6Yb11d6lsiAIDXXnstNRs2bJjZ9vbbbzdzb213bxzeOm5WBvhrDHjXL/Tunf7n7V1/0NraWvJ9A/5c/Dy4xa6q96ZEPylzX4iogni5LFEQLHaiIFjsREGw2ImCYLETBRFmimt7e7uZ33///WZuDcV4Uy29ZYm9aaINDQ1mbg29zZgxw2xrbWsM+FNgvSEoaylr77h8/vnnZu4NCw4ePDg1mzt3rtnWc8stt5j5+++/n+n+K4FndqIgWOxEQbDYiYJgsRMFwWInCoLFThQEi50oiDDj7N5yz96Yr7WU9IgRI8y21nLKgL9tcnNzs5lv3LgxNfOmz3r37fXNO27WVE9vmukTTzxh5i+++KKZr1mzJjVbuHCh2fbQoUNmfv3115s5x9mJKDcsdqIgWOxEQbDYiYJgsRMFwWInCoLFThREmHH22tpaM7fmXWfljUV74/Decs27d+9Ozbzlmr356J5+/fqZubWdtDfGf+TIpVsMfpe3TPbhw4dTM6/fqvbmRdbPVVQ8sxMFwWInCoLFThQEi50oCBY7URAsdqIgWOxEQYQZZ58+fbqZe+Oq1li3N17sjaNb65sD9lx6ALjyyivN3OL93P379zdz7xqCBx54IDV77LHHzLYjR44086FDh5r5qFGjUrP9+/ebbb1jPmHCBDMvIvfMLiKvikibiOzocttKEflKRLYm/+6sbDeJKKuePI1/DcD8bm7/jaremPx7p7zdIqJyc4tdVdcDsK9bJKLCy/IG3XIR+SR5mp/64klElorIZhHZnOGxiCijUov9dwAmArgRQCuAX6V9o6o2qupsVZ1d4mMRURmUVOyqelBVz6vqBQAvA5hT3m4RUbmVVOwiUt/ly58C2JH2vURUDO44u4i8DmAegOEisg/ACgDzRORGAAqgGcBDletieYwePdrMvfXVrTXOvbXV169fb+beNQA1NTVmbvXNm6fvXQPQt29fM/euMXjjjTdSs2XLlpltT506ZeZ1dXVmftddd6VmGzZsMNtOnTrVzL29AorILXZVvbebm1+pQF+IqIJ4uSxRECx2oiBY7ERBsNiJgmCxEwURZorr5MmTM7W3lh72poF6Q0RZl3POsgy2N0XVW87Z2wrbGtI8cOCA2fbEiRNm7g0LWlN/Fy1aZLZtamoyc2/Isoh4ZicKgsVOFASLnSgIFjtRECx2oiBY7ERBsNiJgggzzj5t2jQz37Vrl5k3NDSkZt62yOPHjzdzbylob/qtNVbe0dFhtvXG+L2f7dixY2be1taWmnlTgwcMGGDm3jj73r17UzNvCW2P99hFxDM7URAsdqIgWOxEQbDYiYJgsRMFwWInCoLFThREmHF2b7nmIUOGmPnLL7+cmnlzo8+ePWvmWbZcBuyti70ltFtaWszcm1NubWUN2OPR3hi+d32Bt6Wz1TdvPrq1fgHg/06LiGd2oiBY7ERBsNiJgmCxEwXBYicKgsVOFASLnSgIyTqv9wc9mEj1HuwS3tbC3nixtfb7zp07zbbemO2ZM2fM3NseeOvWranZggULzLatra1mfvToUTP3ZFlf3fvbtK4vAIBJkyalZl999VVJfbrIW09/ypQpme4/C1XtdiMB98wuImNE5K8islNEPhWRnye314rIOhHZnXy0jzwR5aonT+M7APxSVa8F8I8AHhaR6wA8CeA9VZ0M4L3kayIqKLfYVbVVVbckn7cD2AmgAcBCAKuTb1sN4O4K9ZGIyuAHXRsvIuMAzATwAYCRqtoKdP6HICLdvqgVkaUAlmbsJxFl1ONiF5FBAP4E4BeqerynmwmqaiOAxuQ+cnuDjii6Hg29iUgfdBb6H1T1z8nNB0WkPsnrAaQvI0pEuXPP7NJ5Cn8FwE5V/XWX6C0ASwCsSj6+WZEelok3vOUtW9ze3p6aWUsWA8DMmTPN3JvK6eXW0NyyZcvMtt5S05XcmtgbWvOmkXrTklesWJGanT592mzrbcN97tw5My+injyNvxnA/QC2i8jW5Lan0Vnkb4jIgwD2ArinIj0korJwi11V/wYg7QX6T8rbHSKqFF4uSxQEi50oCBY7URAsdqIgWOxEQYRZStob0/Wmoe7Zsyc187Yt9njTa73lnK0llV944QWz7ZdffmnmWVnj2d5YtrctsjdNddasWamZ9/dQW1tr5gcPHjTzIuKZnSgIFjtRECx2oiBY7ERBsNiJgmCxEwXBYicKIsw4uzf/eNCgQWZuzYf3lmO++uqrzXzbtm1m7m1tbC337C153Lu3/SeQdVtla6591rn0U6dONfNnn302NVu+fLnZ9oMPPjDz2267zcyLiGd2oiBY7ERBsNiJgmCxEwXBYicKgsVOFASLnSiIMFs2e2PZ1pbMgL2uvLd++caNG828vr7ezL2xcGt3nqy/3wsXLpj54MGDzfyRRx5JzebPn2+2Xbx4sZkvWrTIzNeuXZuabdq0yWz7+OOPm/m7775r5t76CJVU8pbNRPTjwGInCoLFThQEi50oCBY7URAsdqIgWOxEQbjj7CIyBsDvAYwCcAFAo6r+VkRWAvgXAIeSb31aVd9x7iu3cfYNGzaY+YQJE8z8+PHjqdmUKVNK6tNF48aNM/PZs2eX3H769OlmW29fem++ure2uzdWnpeWlhYz964f2Ldvn5l7x72S0sbZe7J4RQeAX6rqFhG5CsBHIrIuyX6jqv9Rrk4SUeX0ZH/2VgCtyeftIrITQEOlO0ZE5fWDXrOLyDgAMwFcXLNnuYh8IiKvisjQlDZLRWSziGzO1lUiyqLHxS4igwD8CcAvVPU4gN8BmAjgRnSe+X/VXTtVbVTV2apqv/AkoorqUbGLSB90FvofVPXPAKCqB1X1vKpeAPAygDmV6yYRZeUWu3ROqXoFwE5V/XWX27tO1fopgB3l7x4RlUtP3o2/GcD9ALaLyNbktqcB3CsiNwJQAM0AHqpA/8qmT58+Zj50aLdvOfydtS1yVs3NzZly+uG8KajekKQ3NFdEPXk3/m8Auhu3M8fUiahYeAUdURAsdqIgWOxEQbDYiYJgsRMFwWInCiLMls0LFiwwc2+K6+nTp8vZne/wlor2piFbyz1Xeqlwaxnrajx+qZ555hkznzFjhplv2bKlnN2pCp7ZiYJgsRMFwWInCoLFThQEi50oCBY7URAsdqIgqr1l8yEAX3a5aTiAr6vWgR+mqH0rar8A9q1U5ezbNao6orugqsX+vQcX2VzUtemK2rei9gtg30pVrb7xaTxRECx2oiDyLvbGnB/fUtS+FbVfAPtWqqr0LdfX7ERUPXmf2YmoSljsREHkUuwiMl9E/ldEvhCRJ/PoQxoRaRaR7SKyNe/96ZI99NpEZEeX22pFZJ2I7E4+2gveV7dvK0Xkq+TYbRWRO3Pq2xgR+auI7BSRT0Xk58ntuR47o19VOW5Vf80uIr0A7ALwzwD2AdgE4F5V/ayqHUkhIs0AZqtq7hdgiMhcACcA/F5Vr09u+3cAR1R1VfIf5VBV/deC9G0lgBN5b+Od7FZU33WbcQB3A3gAOR47o1+LUYXjlseZfQ6AL1S1SVW/BfBHAAtz6Efhqep6AEcuuXkhgNXJ56vR+cdSdSl9KwRVbVXVLcnn7QAubjOe67Ez+lUVeRR7A4CWLl/vQ7H2e1cAfxGRj0Rkad6d6cZIVW0FOv94ANTl3J9Ludt4V9Ml24wX5tiVsv15VnkUe3eLlhVp/O9mVf0HAHcAeDh5uko906NtvKulm23GC6HU7c+zyqPY9wEY0+XrqwHsz6Ef3VLV/cnHNgBrUbytqA9e3EE3+diWc3/+rkjbeHe3zTgKcOzy3P48j2LfBGCyiIwXkSsB/AzAWzn043tEZGDyxglEZCCA21G8rajfArAk+XwJgDdz7Mt3FGUb77RtxpHzsct9+3NVrfo/AHei8x35/wPwb3n0IaVfEwBsS/59mnffALyOzqd159D5jOhBAMMAvAdgd/KxtkB9+y8A2wF8gs7Cqs+pb7eg86XhJwC2Jv/uzPvYGf2qynHj5bJEQfAKOqIgWOxEQbDYiYJgsRMFwWInCoLFThQEi50oiP8HMjgjSZdKCisAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:6\n"
     ]
    }
   ],
   "source": [
    "# iterate through the DataLoader\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# show one from the batch\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap = 'gray')\n",
    "plt.show()\n",
    "print(f\"label:{label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd43169-eab5-415a-a705-78e0cdc4d434",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901658fa-0659-45ab-97b4-f33ee00fdc1d",
   "metadata": {},
   "source": [
    "Use transforms to perform some manipulation of the data and make it suitable for training.\n",
    "\n",
    "what:\n",
    "\n",
    "All TorchVision datasets have two parameters -\n",
    "1. transform to modify the features. \n",
    "2. target_transform to modify the labels.\n",
    "\n",
    "Why:\n",
    "\n",
    "We need the features as normalized tensors, and the labels as one-hot encoded tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa3cdeca-3ce8-4116-892a-929ebdcbb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    # Convert a PIL image or Numpy ndarray into a FloatTensor,and scales the image's pixel intensity values in the range [0.,1.]\n",
    "    transform = ToTensor(),\n",
    "    # create a zero tensor of size 10, and calls scatter_ which assigns a value=1 on the index given by y.\n",
    "    target_transform = Lambda(lambda y: torch.zeros(10, dtype = torch.float).scatter_(0,torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f0c17-24a7-48be-8152-885dd1a6f9b7",
   "metadata": {},
   "source": [
    "The transformations accept both tensor images and batches of tensor images.\n",
    "\n",
    "* Tensor images (C, H, W) where C means channels\n",
    "* A batch of tensor images (B, C, H, W) where B is the number of images in the batch\n",
    "\n",
    "Torchvision.transform will apply the same transformation to all the images of a given batch, but reproducible transformations need functional transforms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2fb81-9cd0-4f75-a64f-62f9b427999a",
   "metadata": {},
   "source": [
    "ToTensor():\n",
    "\n",
    "Convert a PIL image or Numpy ndarray into a FloatTensor,and scales the image's pixel intensity values in the range [0.,1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05625da-422e-488d-8cce-dae9cc2ba280",
   "metadata": {},
   "source": [
    "Lambda Transforms:\n",
    "\n",
    "Apply any user-defined lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d779d1-9720-4831-919a-08c67b61a2b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240746f-2073-407f-b7e4-7e4ec0697ec6",
   "metadata": {},
   "source": [
    "#### A simple DFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5670b7-8c1e-4cb3-acb6-7a4189060a35",
   "metadata": {},
   "source": [
    "Given by torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754cae54-69d1-4dde-9932-4becdef93294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c75ca2-2401-4c14-9842-a841ea043cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# get device for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60548dcd-2d41-4939-a955-234efea615cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Class\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "# nn.Module: Base class for all neural network modules.\n",
    "\n",
    "# nn.Linear():in_features, out_features, bias\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # calling the model on the inp\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # input image size\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            # output raw predicted values for each class. A softmax is needed to gain the probabilities.\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86944c3-96ed-4e62-abb7-093de3daa38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create an instance and move it to the device\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c21ffa-76f1-4466-9fa5-32928af82ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass a random input data, and it executes the model's forward.\n",
    "X = torch.rand(1, 28, 28, device = device)\n",
    "logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c034197-ba60-48c1-9eaf-7145b785838f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0348, 0.0000, 0.0532, 0.0975, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222f2381-c89b-4405-88de-4350f65b97ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# nn.Softmax: dim: A dimension along which Softmax will be computed (so every slice along dim will sum to 1).\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "pred_probab.argmax(1)\n",
    "# argmax(dim to keep) https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dba506-036c-4a30-a402-141e95ed9694",
   "metadata": {},
   "source": [
    "#### Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87eb4a5f-cfb5-4b23-8b45-49e68b5e48af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# flatten layer convert each 2D 28*28 image into a contiguous array of 784 pixel values\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679a86cf-3576-4983-8f34-3004b1f723d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# linear layer is a module that applies a linear transformation on the input and stored weights and biases.\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad8ba04-2b0a-4c0b-b453-b4fead7fd4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: \n",
      "tensor([[-3.1706e-01, -1.5662e-01, -1.3143e-01,  6.7908e-02,  1.5360e-01,\n",
      "         -2.7691e-01,  5.1282e-01, -2.3271e-04, -4.2544e-01, -3.5582e-01,\n",
      "          3.8420e-01, -2.0431e-01,  2.7121e-01, -7.6910e-02, -9.6500e-02,\n",
      "          1.4603e-01,  7.5897e-01, -2.1396e-02, -1.7443e-01,  3.8205e-01],\n",
      "        [-2.4036e-01, -2.6383e-01,  2.5190e-02, -1.6716e-01, -7.1717e-02,\n",
      "         -6.2537e-02,  2.7260e-01, -1.6416e-01, -1.4421e-01, -1.9516e-01,\n",
      "          4.2799e-01, -2.2483e-01, -9.1911e-02,  1.3871e-02,  5.2699e-01,\n",
      "          2.5722e-01,  6.6322e-01, -1.9219e-01, -1.7133e-01,  4.0427e-01],\n",
      "        [-1.1872e-01, -1.7311e-01,  3.1229e-01, -3.7746e-01, -2.4232e-01,\n",
      "         -6.0705e-02,  3.2630e-01, -1.0507e-01, -1.3016e-01, -3.3706e-01,\n",
      "          1.5292e-01, -1.3960e-01,  2.0433e-01,  4.7560e-01,  5.6245e-01,\n",
      "          4.3806e-01,  4.6230e-01,  1.1684e-01,  5.7419e-02,  3.9399e-01]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0679, 0.1536, 0.0000, 0.5128, 0.0000, 0.0000,\n",
      "         0.0000, 0.3842, 0.0000, 0.2712, 0.0000, 0.0000, 0.1460, 0.7590, 0.0000,\n",
      "         0.0000, 0.3821],\n",
      "        [0.0000, 0.0000, 0.0252, 0.0000, 0.0000, 0.0000, 0.2726, 0.0000, 0.0000,\n",
      "         0.0000, 0.4280, 0.0000, 0.0000, 0.0139, 0.5270, 0.2572, 0.6632, 0.0000,\n",
      "         0.0000, 0.4043],\n",
      "        [0.0000, 0.0000, 0.3123, 0.0000, 0.0000, 0.0000, 0.3263, 0.0000, 0.0000,\n",
      "         0.0000, 0.1529, 0.0000, 0.2043, 0.4756, 0.5624, 0.4381, 0.4623, 0.1168,\n",
      "         0.0574, 0.3940]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# No-linear activations will create the complex mappings between the model's inputs and outputs, helping neural networks learn a wide variety of phenomena.\n",
    "# ReLU(z)=max(0, z)\n",
    "print(f\"Before ReLU: \\n{hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: \\n{hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a45083ae-be42-4877-90f5-dbe00a63c646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0981, 0.0981, 0.1016, 0.0981, 0.1035, 0.1082, 0.0981, 0.0981, 0.0981,\n",
       "         0.0981]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## nn.Softmax is the last layer of the neural network which returns logits \n",
    "# logits: raw values in [-infty, infty]\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probad = softmax(logits)\n",
    "pred_probad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa02e1f-0d07-458a-816a-952d9426a5d1",
   "metadata": {},
   "source": [
    "#### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f824e-bf98-4d9f-a4d1-2503ee7164cf",
   "metadata": {},
   "source": [
    "nn.Module automatically tracks all fields defined inside models.\n",
    "\n",
    "accessed by model's parameters() or named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36436819-0953-4fb1-b9b4-894467104657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784])\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512])\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512])\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512])\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512])\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3262c-e823-4988-bfac-6c65b2f0bd1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "![img_weights](./img/weights.png \"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7eb8cc-e184-4e30-a5ca-3f70a4eef0de",
   "metadata": {},
   "source": [
    "### Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab3f2d-b09d-45a2-a8a4-f26e72a4effc",
   "metadata": {},
   "source": [
    "* Back Propagation are adjusted according to the gradient of the loss function with respect to the given parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe7b400a-b6d2-4b2b-8950-d836161384a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True) # requires_grad is used to denote parameters to learn\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x,w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97902729-7893-4b3a-9048-af220032e000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f4902a75310>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f4902a75430>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34369e38-ad95-410e-a011-f47bfbde6279",
   "metadata": {},
   "source": [
    "#### Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020bb0f-e5aa-45ba-92ea-8e6889a882b5",
   "metadata": {},
   "source": [
    "Compute derivatives of loss function respect to parameters in different layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0f2bf96-8f8d-4090-8c26-2890ce70fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0372, 0.0525, 0.2464],\n",
      "        [0.0372, 0.0525, 0.2464],\n",
      "        [0.0372, 0.0525, 0.2464],\n",
      "        [0.0372, 0.0525, 0.2464],\n",
      "        [0.0372, 0.0525, 0.2464]])\n",
      "tensor([0.0372, 0.0525, 0.2464])\n"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph = True)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a409329-7c94-482f-8bef-86a336f6c525",
   "metadata": {},
   "source": [
    "http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3a255-ac5e-4d22-b8e5-be53342ab0b7",
   "metadata": {},
   "source": [
    "* Gradients will not be available apart from the leaf nodes of the computational graph.\n",
    "\n",
    "* backward() will only perform once. if we need several backward calls, we need to pass retain_graph=True to the backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5a963169-9a6c-4866-a9d3-c8f19afcfed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5662,  3.6226, 17.0004],\n",
      "        [ 2.5662,  3.6226, 17.0004],\n",
      "        [ 2.5662,  3.6226, 17.0004],\n",
      "        [ 2.5662,  3.6226, 17.0004],\n",
      "        [ 2.5662,  3.6226, 17.0004]])\n",
      "tensor([ 2.5662,  3.6226, 17.0004])\n"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph = True)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e7b98-1459-42fe-a9d5-0a359bb0e6f9",
   "metadata": {},
   "source": [
    "all tensors with require_grad=True are tracking their computational history and support gradient computation.\n",
    "\n",
    "But in some cases, like, with the trained model, there's no need to do back propagation as only forward computations are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9f2dd9c-2e95-4342-ad11-1c00e080e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952cab20-de46-41dc-a64f-164562f1cc0b",
   "metadata": {},
   "source": [
    "Cases to ues:\n",
    "\n",
    "* Funetuning a pretrained network\n",
    "* speed up computations when only doing forward pass, and it will happens on tensors that do not track gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf09ddd-88f6-473e-b63e-60b1c7ca6555",
   "metadata": {},
   "source": [
    "#### More on Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b93124-b9c7-4e36-857a-bc70830442f0",
   "metadata": {},
   "source": [
    "* website:\n",
    "https://youtu.be/MswxJw-8PvE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89b367-6aa2-4151-a888-474dd16005a4",
   "metadata": {},
   "source": [
    "### Optimizing model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d081451-a2b9-45bc-ad16-5a7b30d11ce2",
   "metadata": {},
   "source": [
    "Training a model is an iterative process. Each iteration is called an epoch, during which the model make a guess about the output, calculates the error in its guess(loss).\n",
    "\n",
    "Collect the derivatives of the error with respect to its parameters, and optimizes the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d0b8f0-3ebd-4fbb-91ae-7e1ef365f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\", # show the path where train/test data is stored\n",
    "    train = True, # specify training or test dataset\n",
    "    download = True, # download the data from the internet if it's not available at root\n",
    "    transform = ToTensor() # specify the feature and label transformations\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\", # show the path where train/test data is stored\n",
    "    train = False, # specify training or test dataset\n",
    "    download = True, # download the data from the internet if it's not available at root\n",
    "    transform = ToTensor() # specify the feature and label transformations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212f54b6-bafd-4cac-b7e8-a05c7775f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8302eb-08d8-4ac3-b5b7-4f7b129150b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796be712-0754-4969-a2c5-f33b6ca1a1d9",
   "metadata": {},
   "source": [
    "What:\n",
    "\n",
    "Hyperparameters are ajustable parameters that let you control the model optimization process.\n",
    "(impact model training and convergence rates)\n",
    "\n",
    "* Number of Epochs: the number of times to iterate over the dataset\n",
    "* Batch Size: the number of data samples propagated through the network before the parameters are updated\n",
    "* Learning Rate: how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while may result in unpredictable behavior during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5c232b-40ba-4ea9-b96b-200100dff425",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5350ea9-8bbc-45df-8f4e-46224beda221",
   "metadata": {},
   "source": [
    "#### Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390f1fe-6280-48a6-9631-4373c24b5490",
   "metadata": {},
   "source": [
    "Each iteration of the optimization loop is called an epoch. \n",
    "\n",
    "Each epoch consists of two main parts:\n",
    "* The Train Loop: iterate over the training dataset and try to converge to optimal parameters\n",
    "* The Validation/Test Loop: iterate over the test dataset to check if model performance is improving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0acf091-58db-4696-925a-8590a9d7a5fd",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19650a2b-5d17-4490-bfcd-ea7881499e73",
   "metadata": {},
   "source": [
    "Measure the degree of dissimilarity of obtained result with the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8be5b6-6d3c-4ce6-9665-29913546eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb9040-a4cd-4264-9940-2f6604b43de9",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3dd86a-b43f-4fb0-9472-5ceb46b3c224",
   "metadata": {},
   "source": [
    "Optimization is the process of adjusting model parameters to reduce model error in each training step.\n",
    "\n",
    "Optimization algorithms(like Stochastic Gradient Descent) defines how this process is performed.\n",
    "\n",
    "Different Optimizers such as ADAM and RMSProp that work better for different kinds of models and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53109abe-70ae-4bb2-9ef4-27e530251cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
